---
title: 'Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models - DRAFT'
date: 2024-05-15
permalink: /posts/2024/Uni-ControlNet/
tags:
  - Generative artificial intelligence
  - Text-to-Image generation
  - Diffusion models
---

Generative artificial intelligence has made tremendous progress over the last two years.
Since the recent success of ChatGPT, some of these models have found their way into everyday life and are no longer just a subject of research.
Alongside chatbots, the most attention was paid to Text-to-Image models like DALL-E, or Stable Diffusion.
These are able to generate realistic and detailed images using a text prompt given by the user.

In this blog post, I intend to explain a common weakness of Text-to-Image generation models and explore one possible solution following the introduction of Uni-ControlNet.

Why do we need more control in Text-to-Image models?
======

Text-to-Image models are great at generating a detailed image that adheres to the specifications of a simple text prompt.
But if we want to create a specific image, it can be hard to write a text prompt that accurately describes our needs and results in the desired image being generated.

Let's say we really like this wallpaper of a Lamborghini driving on a racetrack and want to recreate the same image with our favorite car.
We can start with a simple prompt and let the model generate an image.
Then we can add more details to the text prompt and try to influence the image generation to make the resulting image look more like our reference image.
We find that even with a very detailed description, it is difficult to directly influence the generated image.
Furthermore, the models face challenges in understanding complex text prompts.
In our example, the created images show a car taking a right corner, even though the prompt specified that the car should be taking a left turn. 

The above mentioned reasons show, that there is a need for more control beyond text descriptions.
This is the reason why Uni-ControlNet and similar technologies were developed.

![Wallpaper of a lamborghini](/images/lamborghini_wallpaper.jpg){: .align-center width="450px"}

I don't know how to format these pictures :(

![Image of LaFerrari 1](/images/laferrari_image_1.jpeg){: .align-left width="200px"}
<figcaption>Create an image of a red LaFerrari driving towards the camera</figcaption>

![Image of LaFerrari 2](/images/laferrari_image_2.jpeg){: .align-left width="200px"}
<figcaption>A red LaFerrari driving towards the camera. The car is driving on a racetrack and is taking a left turn.</figcaption>

![Image of LaFerrari 3](/images/laferrari_image_3.jpeg){: .align-left width="200px"}
<figcaption>Create an image of a red LaFerrari driving towards the camera. The car is driving on a racetrack and is in a left turn. 
In the background, you can see two modern-looking buildings and stacked tires that are painted red and white.</figcaption>


What is Uni-ControlNet?
======

Uni-ControlNet is a framework that allows the utilization of local and global controls.
It works with pre-trained text-to-Image diffusion models. In this case Stable Diffusion is used.
Only requires fine-tuning of two adapters.

![Motivation controls](/images/motivation_controls.png){: .align-center width="800px"}


What are diffusion models and how do they work?
======

Diffusion models are a very recent development in the field of generative AI. 
They were first proposed in 2015, but found broudt adaption only in the last two years.
They are used in cutting edge Text-to-Image models like DALL-E and Stable Diffusion.
Diffusion models are inspired by thermodynamics.

![Number of papers diffusion](/images/number_of_papers_diffusion.png){: .align-center width="500px"}

Diffusion models work, by first systematically and slowly destroying structure in the training data.
Since we are talking about image generation, the training data is a collection of images.
We can destroy structure in these images by applying a low-level noise.
This process is called Forward Diffusion.
Then we try to train a model that can reverse the process and recovers structure in data.
We call this the Reverse Diffusion Process.
At inference time we can generate images by passing random noise through recovering process
and using the text prompt to create a new image.

![Diffusion overview](/images/diffusion_overview.png){: .align-center width="500px"}

Forward Diffusion Process
------

The Forward Diffusion process consists of multiple steps of adding Gaussian noise to the input image.
The scale of noise varies at each step.
by applying noise the training data is progressively destroyed.
At the end of this process only static noise remains.

![Forward diffusion process](/images/forward_diffusion_process.png){: .align-center width="500px"}

Reverse Diffusion Process
------

The goal of the Reverse Diffusion Process is, to train a model to predict the noise in a given image.
We can then subtract the noise from the image to recover more of the original image.
We repeat this process multiple steps until we have completly reversed the diffusion process.
Now we have a model that can recover images from randm noise.

![Reverse diffusion process](/images/reverse_diffusion_process.png){: .align-center width="500px"}

Diffusion Inference
------

To create a new image, we start with random white noise.
We then gradually try to reconstruct a image by removing noise using the Reverse Diffusion Process.
Use text embeddings from user prompt.

![Diffusion inference](/images/diffusion_inference.png){: .align-center width="500px"}


How does Uni-ControlNet work?
======

Uni-Controlnet doesn't change the base diffusion process.
It adds two adapters to the image generation process.
One adapter is used for global control and the other is used for local control.
the global control adapter takes the text prompt and the global conditions and produces an extendet promt.
This extendet prompt is used in the further image generation.
Let's take a closer look at how the adapters work.

![Framework Uni-ControlNet](/images/framework_uni-controlnet.png){: .align-center width="500px"}

Global Control Adapter
------

Global control is realized with a reference image.
Provides a more nuanced understanding of the semantic content.

![Global control example](/images/global_control_example.png){: .align-center width="500px"}

Extract image embeddings using CLIP (image encoder by OpenAI)
Image embeddings encode the contents of an image
Reshape the embeddings into k tokens

Concatenate the text prompt with the condition tokens
Result is extended prompt

![Global control adapter](/images/global_control_adapter.png){: .align-center width="500px"}

![Weight of global condition](/images/weight_of_global_condition.png){: .align-center width="500px"}

Local Control Adapter
------

Uni-ControlNet supports seven different local conditions

![Local control adapters](/images/local_control_adapters.png){: .align-center width="500px"}

Concatenate different local conditions and extract features
Copy the structure and weights of the encoder
Inject the condition information at different resolutions

![Local control adapter](/images/local_control_adapter.png){: .align-center width="500px"}

Information from local control adapter is usedduring decoding
The two adapters are trained separately and merged at inference time

Composition of conditions
------

Multiple local and one global condition can be combined

![Composition of conditions](/images/composition_of_conditions.png){: .align-center width="700px"}


Comparison with related work
======

In the following Uni-NetControl is compared to other state-of-the-art control frameworks for Text-to-Image generation.
These are ControlNet, GLIGEN and T2I-Adapters.
For the given text prompt and local conditions the frameworks product different results.

![Comparison of quality](/images/comparison_quality.png){: .align-center width="700px"}

Extract conditions from natural and generated image
Compare using SSIM (Structural Similarity)

![Comparison of quality table](/images/comparison_quality_table.png){: .align-center width="700px"}

To further compare the different frameworks, lets take a look at the cost and model size.
Composer requires training the model from scratch.
ControlNet, GLIGENandT2I-Adapter need one independent adapter for each condition.
Composability of different conditions not possible with GLIGEN.

![Comparison of cost table](/images/comparison_cost_table.png){: .align-center width="700px"}


Conclusion
======

Uni-ControlNet enables integration of local and global controls.
Simultaneousutilizationofmultipleconditionsispossible.
Surpasses ControlNet, GLIGEN and T2I-Adapter in controllability and quality.
Uni-ControlNet only requires fine-tuning of two adapters.

