---
title: 'Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models'
date: 2024-07-20
permalink: /posts/2024/Uni-ControlNet/
tags:
  - Generative artificial intelligence
  - Text-to-Image generation
  - Diffusion models
---

Artificial intelligence (AI) has made tremendous progress over the last two years and has received a lot of attention from the media and the public.
The subcategory of AI that has contributed most to this development is generative artificial intelligence.
Generative AI models are capable of generating new data (usually text, images, or videos) in response to user-given prompts.
Starting with the recent success of ChatGPT, some of these models have found their way into everyday life and are no longer just a subject of research.
Alongside chatbots, the most frequently used generative AI applications are Text-to-Image models like DALL-E[^1], or Stable Diffusion[^2].
These are able to generate realistic and detailed images using textual instructions given by the user.

In this blog post, I intend to explain a common weakness of Text-to-Image generation models and explore one possible solution following the introduction of Uni-ControlNet[^3].


Why do we need more control in Text-to-Image models?
======

Text-to-Image models are great at generating detailed images that adhere to the specifications of a simple prompt.
The problem is, if we want to create one specific image, it can be hard to write a text prompt that accurately describes our needs and results in the desired image being generated.

Let's say, for example, that we like this wallpaper of a Lamborghini driving on a racetrack and want to recreate a similar picture with a different car.
We can start with a simple text prompt and let the model generate the first image.
Then we add more details to the prompt and try to influence the image generation to make the resulting image look more like our reference wallpaper.
If we compare the reference image with our final generated image, we notice that there are still some major differences:
The number and appearance of the buildings in the background differ, and the stacked tires are painted in different ways.
We find that even with a very detailed description, it is difficult to directly influence the generated image.
Furthermore, the models face challenges in understanding complex text prompts.
In our example, the created images show a car making a right-hand turn, even though the prompt specified that the car should be taking a left turn.

Another potential problem, not shown in our example, are ambiguous sentences like "John watched the man with the telescope".
This could either mean that John was using the telescope to watch the man or that John was watching the man while the man was using the telescope.
A Text-to-Image model may choose the wrong interpretation of such a sentence.

The above-mentioned reasons show that there is a need for more control beyond text descriptions.
For this purpose Uni-ControlNet and similar technologies were developed.

<style>
table th:first-of-type {
    width: 25%;
}
table th:nth-of-type(2) {
    width: 25%;
}
table th:nth-of-type(3) {
    width: 25%;
}
table th:nth-of-type(4) {
    width: 25%;
}
</style>

| Reference image | Prompt 1  | Prompt 2 | Prompt 3 |
| :---------------: | :--------: | :-------: | :--------: |
| ![Wallpaper of Lamborghini](/images/lamborghini_wallpaper.jpg){: .align-center width="450px"}| ![Image of LaFerrari 1](/images/laferrari_image_1.jpeg){: .align-center width="200px"} | ![Image of LaFerrari 2](/images/laferrari_image_2.jpeg){: .align-center width="200px"} | ![Image of LaFerrari 3](/images/laferrari_image_3.jpeg){: .align-center width="200px"} |
| **The reference image we are trying to recreate.** | "**A red LaFerrari driving towards the camera.**" | "A red LaFerrari driving towards the camera. **The car is driving on a racetrack and is taking a left turn.**" | "A red LaFerrari driving towards the camera. The car is driving on a racetrack and is taking a left turn. **In the background, you can see two modern-looking buildings and stacked tires that are painted red and white.**"|


What is Uni-ControlNet?
======

Uni-ControlNet is a framework that allows for the utilization of different conditions beyond text descriptions.
These additional controls are categorized into local and global controls.
Local controls define the structure of one or more specific parts of a picture.
Down below, you can see an example of a local condition being used to control the generation of an image.
The prompt "A man running on the street" is extended with an additional condition that describes the position of his joints and body parts.
The model now tries to generate an image that fulfills both the prompt and the structure given by the local condition.
In contrast to local controls, global controls influence the overall picture without directly controlling specific details.
An example of a global condition is a reference image that the diffusion model takes into account.

Uni-ControlNet works with pre-trained Text-to-Image diffusion models. 
An existing model can be augmented with Uni-ControlNet without changing the model itself.
This expansion is realized by using two adapters, one for local and one for global control.
We will take a closer look at the functionality of the different adapters later.
In the implementation of Uni-ControlNet considered here, the Text-to-Image model Stable Diffusion is used, although the same methods are applicable to other diffusion models.

![Motivation controls](/images/motivation_controls.png){: .align-center width="800px"}


What are diffusion models and how do they work?
======

Diffusion models are a recent development in the field of generative AI. 
They were first introduced in 2015 but have only become established in the last three years[^4].
Diffusion models are inspired by thermodynamics and are used in cutting-edge Text-to-Image models like DALL-E and Stable Diffusion.

<p align="center">
    <img src="/images/number_of_papers_diffusion.png" width="500px">
    <br>
    <sup>The rough number of papers on diffusion models per year (<a href="https://arxiv.org/pdf/2209.04747">source</a>)</sup>
</p>

Diffusion models work by first systematically and slowly destroying structure in the training data.
Since we are talking about image generation, the training data is a collection of images.
We can destroy the structure in these images by applying low-level noise to them.
This procedure is called *Forward Diffusion Process*.
Then we try to train a model that can reverse the process and recover structure from the corrupted data.
We call this step *Reverse Diffusion Process*.
At inference time, we can generate images by passing random noise through the recovering process and using a text prompt to create a new image.
In the following section, we will take a more detailed look at each of the different steps.

![Diffusion overview](/images/diffusion_overview.png){: .align-center width="500px"}

Forward Diffusion Process
------

The Forward Diffusion Process consists of multiple steps of adding Gaussian noise to a image from the training data.
Gaussian noise is a kind of signal noise that has a probability density function equal to that of the normal distribution.[^5]
The scale of noise varies at each step.
By applying noise, the training data is progressively destroyed.
At the end of this process, only pure Gaussian noise remains.

![Forward diffusion process](/images/forward_diffusion_process.png){: .align-center width="600px"}

Reverse Diffusion Process
------

The goal of the Reverse Diffusion Process is to train a model to predict the noise in a given image.
We can then subtract the noise from the image to recover more of the original image.
This process is repeated multiple times until we have completely reversed the diffusion process.
Now we have a model that can recover images from random noise.

![Reverse diffusion process](/images/reverse_diffusion_process.png){: .align-center width="600px"}

Diffusion Inference
------

To create a new image, we start with random white noise.
We then gradually try to reconstruct an image by removing noise using the Reverse Diffusion Process.
Text embeddings are extracted from the user prompt and used for image generation.

![Diffusion inference](/images/diffusion_inference.png){: .align-center width="500px"}


How does Uni-ControlNet work?
======

Uni-ControlNet doesn't change the base diffusion process.
It adds two adapters to the image generation process, one for local and one for global control.
The global control adapter takes the text prompt and the global condition and produces an extended prompt.
The local control adapter uses noise, the local conditions, and the extended prompt to produce an output that is combined with the output from the Diffusion Encoder.
Let's take a closer look at how the adapters work.

![Framework Uni-ControlNet](/images/framework_uni-controlnet.png){: .align-center width="800px"}

Global Control Adapter
------

Global control is realized with a reference image.
This provides a more nuanced understanding of the semantic content.

![Global control example](/images/global_control_example.png){: .align-center width="500px"}

At first image embeddings are extracted from the reference image using a tool called CLIP.
CLIP (Contrastive Language-Image Pre-training) is an image encoder developed by OpenAI.
Image embeddings encode the contents of an image.
We then reshape these embeddings into k tokens.

The tokens from the text prompt are concatenated with the condition tokens.
This results in the extended prompt.

![Global control adapter](/images/global_control_adapter.png){: .align-center width="500px"}

The global control adapter also enables the user to control how strong the reference image can influence the generated image.
This is implemented with the hyper parameter lambda.

![Weight of global condition](/images/weight_of_global_condition.png){: .align-center width="500px"}

Local Control Adapter
------

Uni-ControlNet supports seven different local conditions.
These conditions are: Canny edge, MLSD edge, HED boundary, sketch, Openpose, Midas depth, and segmentation mask.

![Local control adapters](/images/local_control_adapters.png){: .align-center width="700px"}

The local control adapter starts by concatenating the different local conditions.
After this, the Feature Extractor H is used to extract features.
The structure and weights of the encoder from the diffusion model are copied and also used in this adapter.
The condition information is injected at different resolutions into the copied encoder.

![Local control adapter](/images/local_control_adapter.png){: .align-center width="500px"}

Information from the local control adapter is combined with the output of the encoder and used during decoding.
The two adapters are trained separately and merged at inference time.

Composition of conditions
------

Multiple local and one global condition can be combined and are simultaneously respected by the diffusion model.
This doesn't work for all combinations of local conditions because they may be contradictory.

![Composition of conditions](/images/composition_of_conditions.png){: .align-center width="700px"}

It is also possible to haven different local conditions that contradict each other.

How does Uni-ControlNet compare with related work
======

In the following, Uni-NetControl is compared to other state-of-the-art control frameworks for Text-to-Image generation.
These are ControlNet, GLIGEN and T2I-Adapter.
For the given text prompt and local conditions, the frameworks produce different results.

![Comparison of quality](/images/comparison_quality.png){: .align-center width="700px"}

To quantify how closely the frameworks followed the given conditions, we first take a natural image and automatically extract our condition (e.g. Canny edge map).
We then take this condition and the corresponding text prompt and create images with each different framework.
From the generated images, we also extract the condition and compare it to the condition of our ground truth.
For this comparison, we use SSIM (Structural Similarity).

![Comparison of quality table](/images/comparison_quality_table.png){: .align-center width="700px"}

To further compare the different frameworks, let's take a look at the cost and model size.
Composer requires training the model from scratch, which requires massive GPU resources and incurs huge training cost.
In contrast ControlNet, GLIGEN and T2I-Adapter only require fine-tuning of lightweight adapters.
They need one independent adapter for each condition, resulting in a linear increase in fine-tuning cost.
The composability of different conditions is not possible with GLIGEN.

![Comparison of cost table](/images/comparison_cost_table.png){: .align-center width="700px"}


Conclusion
======

Uni-ControlNet enables the integration of local and global controls.
It also enables simultaneous utilization of multiple conditions.
Uni-ControlNet surpasses alternatives like ControlNet, GLIGEN, and T2I-Adapter in controllability and quality, while only requiring fine-tuning of two adapters.
It is therefore more cost-effective than other frameworks.


References
======
[^1]: [DALL-E 3](https://openai.com/index/dall-e-3/)
[^2]: [Stable Diffusion 3](https://stability.ai/news/stable-diffusion-3)
[^3]: [Uni-ControlNet](https://arxiv.org/pdf/2305.16322)
[^4]: [Diffusion Models in Vision: A Survey](https://arxiv.org/pdf/2209.04747)
[^5]: [Gaussian noise](https://en.wikipedia.org/wiki/Gaussian_noise)

[^6]: [ControlNet](https://arxiv.org/pdf/2302.05543)
[^7]: [GLIGEN](https://gligen.github.io/)
[^8]: [T2I-Adapter](https://arxiv.org/pdf/2302.08453)

