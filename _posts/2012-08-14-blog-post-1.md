---
title: 'Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models'
date: 2024-07-20
permalink: /posts/2024/Uni-ControlNet/
tags:
  - Generative artificial intelligence
  - Text-to-Image generation
  - Diffusion models
---

Artificial intelligence (AI) has made tremendous progress over the last two years and has received a lot of attention from the media and the public.
The subcategory of AI that has contributed most to this development is generative artificial intelligence.
Generative AI models are capable of generating new data (usually text, images, or videos) in response to user-given prompts.
Starting with the recent success of ChatGPT, some of these models have found their way into everyday life and are no longer just a subject of research.
Alongside chatbots, the most frequently used generative AI applications are Text-to-Image models like DALL-E[^1], or Stable Diffusion[^2].
These are able to generate realistic and detailed images using textual instructions given by the user.

In this blog post, I intend to explain a common weakness of Text-to-Image generation models and explore one possible solution following the introduction of Uni-ControlNet[^3].


Why do we need more control in Text-to-Image models?
======

Text-to-Image models are great at generating detailed images that adhere to the specifications of a simple prompt.
The problem is, if we want to create one specific image, it can be hard to write a text prompt that accurately describes our needs and results in the desired image being generated.

Let's say, for example, that we like this wallpaper of a Lamborghini driving on a racetrack and want to recreate a similar picture with a different car.
We can start with a simple text prompt and let the model generate the first image.
Then we add more details to the prompt and try to influence the image generation to make the resulting image look more like our reference wallpaper.
If we compare the reference image with our final generated image, we notice that there are still some major differences:
The number and appearance of the buildings in the background differ, and the stacked tires are painted in different ways.
We find that even with a very detailed description, it is difficult to directly influence the generated image.
Furthermore, the models face challenges in understanding complex text prompts.
In our example, the created images show a car making a right-hand turn, even though the prompt specified that the car should be taking a left turn.

Another potential problem, not shown in our example, are ambiguous sentences like "John watched the man with the telescope".
This could either mean that John was using the telescope to watch the man or that John was watching the man while the man was using the telescope.
A Text-to-Image model may choose the wrong interpretation of such a sentence.

The above-mentioned reasons show that there is a need for more control beyond text descriptions.
For this purpose Uni-ControlNet and similar technologies were developed.

<style>
table th:first-of-type {
    width: 25%;
}
table th:nth-of-type(2) {
    width: 25%;
}
table th:nth-of-type(3) {
    width: 25%;
}
table th:nth-of-type(4) {
    width: 25%;
}
</style>

| Reference image | Prompt 1  | Prompt 2 | Prompt 3 |
| :---------------: | :--------: | :-------: | :--------: |
| ![Wallpaper of Lamborghini](/images/lamborghini_wallpaper.jpg){: .align-center width="450px"}| ![Image of LaFerrari 1](/images/laferrari_image_1.jpeg){: .align-center width="200px"} | ![Image of LaFerrari 2](/images/laferrari_image_2.jpeg){: .align-center width="200px"} | ![Image of LaFerrari 3](/images/laferrari_image_3.jpeg){: .align-center width="200px"} |
| **The reference image we are trying to recreate.** | "**A red LaFerrari driving towards the camera.**" | "A red LaFerrari driving towards the camera. **The car is driving on a racetrack and is taking a left turn.**" | "A red LaFerrari driving towards the camera. The car is driving on a racetrack and is taking a left turn. **In the background, you can see two modern-looking buildings and stacked tires that are painted red and white.**"|


What is Uni-ControlNet?
======

Uni-ControlNet is a framework that allows for the utilization of different conditions beyond text descriptions.
These additional controls are categorized into local and global controls.
Local controls define the structure of one or more specific parts of a picture.
Down below, you can see an example of a local condition being used to control the generation of an image.
The prompt "A man running on the street" is extended with an additional condition that describes the position of his joints and body parts.
The model now tries to generate an image that fulfills both the prompt and the structure given by the local condition.
In contrast to local controls, global controls influence the overall picture without directly controlling specific details.
An example of a global condition is a reference image that the diffusion model takes into account.

Uni-ControlNet works with pre-trained Text-to-Image diffusion models. 
An existing model can be augmented with Uni-ControlNet without changing the model itself.
This expansion is realized by using two adapters, one for local and one for global control.
We will take a closer look at the functionality of the different adapters later.
In the implementation of Uni-ControlNet considered here, the Text-to-Image model Stable Diffusion is used, although the same methods are applicable to other diffusion models.

![Motivation controls](/images/motivation_controls.png){: .align-center width="600px"}


What are diffusion models and how do they work?
======

Diffusion models are a recent development in the field of generative AI. 
They were first introduced in 2015 but have only become established in the last three years[^4].
Diffusion models are inspired by thermodynamics and are used in cutting-edge Text-to-Image models like DALL-E and Stable Diffusion.

<p align="center">
    <img src="/images/number_of_papers_diffusion.png" width="500px">
    <br>
    <sup>The rough number of papers on diffusion models per year (<a href="https://arxiv.org/pdf/2209.04747">source</a>)</sup>
</p>

Diffusion models work by first systematically and slowly destroying structure in the training data.
Since we are talking about image generation, the training data is a collection of images.
We can destroy the structure in these images by applying low-level noise to them.
This procedure is called *Forward Diffusion Process*.
Then we try to train a model that can reverse the process and recover structure from the corrupted data.
We call this step *Reverse Diffusion Process*.
At inference time, we can generate images by passing random noise through the recovering process and using a text prompt to create a new image.
In the following section, we will take a more detailed look at each of the different steps.

![Diffusion overview](/images/diffusion_overview.png){: .align-center width="500px"}

Forward Diffusion Process
------

The *Forward Diffusion Process* consists of multiple steps of adding Gaussian noise to an image from the training data.
Gaussian noise is a kind of signal noise that has a probability density function equal to that of the normal distribution.[^5]
By applying different scales of noise at each step, the training data is progressively destroyed.
At the end of this process, only pure Gaussian noise remains.

![Forward diffusion process](/images/forward_diffusion_process.png){: .align-center width="600px"}

Reverse Diffusion Process
------

The goal of the *Reverse Diffusion Process* is to train a model to predict the noise in a given image.
We can then subtract the noise from the image to decrease its noise level.
This process is repeated multiple times until we have completely reversed the diffusion process.
Now we have a model that can recover images from random noise.

![Reverse diffusion process](/images/reverse_diffusion_process.png){: .align-center width="600px"}

Inference
------

To generate a new image, we start with random white noise and a text prompt given by the user.
We then gradually try to construct an image by removing noise using the model from the *Reverse Diffusion Process*.
In order for the prompt to have an influence on the generated image, we input a numeric representation of the text into the diffusion model.
To do this, we project the text into a high-dimensional latent space and take the position of our text as a vector.[^6]
The resulting sequence of numbers is called a text embedding.

![Diffusion inference](/images/diffusion_inference.png){: .align-center width="600px"}


How does Uni-ControlNet work?
======

A typical Diffusion Model consists of an encoder, a middle block, and a decoder.
These three components stay unchanged and are extended by two adapters, one for local and one for global control.
The *Global Control Adapter* takes a text prompt and a global condition and uses them to produce an extended prompt.
This extended prompt, together with random noise, is the input to the base Diffusion model and the *Local Control Adapter*.
The *Local Control Adapter* also uses the local conditions to produce an output that is combined with the output from the diffusion encoder.
The two adapters are trained separately and merged at inference time.
Let's take a closer look at how these adapters work.

<p align="center">
    <img src="/images/framework_uni-controlnet.png" width="800px">
    <br>
    <sup>The overall framework of Uni-ControlNet (<a href="https://arxiv.org/pdf/2305.16322">source</a>)</sup>
</p>

Global Control Adapter
------

Global control in Uni-ControlNet is realized with a reference image.
This provides a more nuanced understanding of the semantic content and gives the user indirect control over the appearance of the entire picture.
In this example, the simple prompt "Golden retriever" and a reference image of a landscape are used as the input for the Diffusion Model.
As you can see, the reference image is not directly used in the output, but the resulting image closely resembles the global condition.

![Global control example](/images/global_control_example.png){: .align-center width="600px"}

The Global Control Adapter works by first extracting image embeddings from the reference image.
Just as text embeddings are a numerical representation of text, image embeddings are a numerical representation of images and encode information about the contents of an image.
These embeddings are extracted using a tool called CLIP[^7].
CLIP stands for *Contrastive Language-Image Pre-training* and is an image encoder developed by OpenAI.
The image embeddings are then reshaped into text and split into tokens.
The text prompt is also split into tokens and is concatenated with the condition tokens.
This results in an extended prompt.

![Global control adapter](/images/global_control_adapter.png){: .align-center width="600px"}

The Global Control Adapter also enables the user to control how strongly the reference image influences the generated image.
This is implemented using a hyper-parameter ÊŽ that controls the weight of the global condition.
This parameter can take values from 0.5, where the global condition is not taken into account, to 1.0, where the text prompt is not used and the image only depends on the global condition.
Below, you can see two examples with the prompts "Forest" and "City" and one global condition each.

![Weight of global condition](/images/weight_of_global_condition.png){: .align-center width="600px"}

Local Control Adapter
------

Local controls define the structure of one or more specific parts of an image.
Uni-ControlNet supports seven different local conditions.
These conditions are: Canny edge, MLSD edge, HED boundary, sketch, Openpose, Midas depth, and segmentation mask.
Several of the same or different conditions can be used at the same time. 

![Local control adapters](/images/local_control_adapters.png){: .align-center width="700px"}

The Local Control Adapter starts by concatenating all used local conditions.
After this, a feature extractor is used to extract all features and important information from the concatenated conditions.
At the next step, a copy of the encoder from the base Diffusion Model is used.
The condition information is injected at different resolutions into the copied encoder.
The copied encoder uses noise, the extended prompt, and the injected condition information to produce an output, which is combined with the output of the base encoder and used during decoding.

![Local control adapter](/images/local_control_adapter.png){: .align-center width="450px"}

Composition of conditions
------

Multiple local and one global condition can be combined and simultaneously used by the Diffusion Model.
Because contradictory conditions are possible, this does not always produce good results.
The first example shows the combination of the two local conditions, Openpose and segmentation mask.
These do not contradict each other and can therefore be used without any problems to create an image.
In the second example, you can see an edge and a depth map that show different things.
The model tries to fulfill both conditions but is not able to create a sensible image.
In other cases, one of the two contradictory conditions may be completely ignored.

![Composition of conditions](/images/composition_of_conditions.png){: .align-center width="600px"}


How does Uni-ControlNet compare with related work?
======

In the following section, Uni-ControlNet is compared to other state-of-the-art control frameworks for Text-to-Image generation.
These are ControlNet[^8], GLIGEN[^9] and T2I-Adapter[^10].

For the given text prompts and local conditions, the frameworks produce different results.
At first glance, it looks as if they all equally meet the conditions.

![Comparison of quality](/images/comparison_quality.png){: .align-center width="700px"}

To quantify how closely the frameworks followed the given conditions, we first take a natural image and automatically extract our condition (e.g. Canny edge map).
We then take this condition and the corresponding text prompt and create one image for each different framework.
From the generated images, we also extract the condition and compare it to the condition of our natural image.
For this comparison, we use the structural similarity index measure (SSIM)[^11].
The underlying idea is that if the model adheres completely to the given condition, the resulting image should contain exactly the same condition.

In the table below, you can see that Uni-ControlNet performs really well against the alternative frameworks.
For some framework condition combinations, there are no values available because the given framework doesn't support this condition.

![Comparison of quality table](/images/comparison_quality_table.png){: .align-center width="700px"}

To further compare the different frameworks, let's take a look at the cost of creating and modifying them.
We therefore compare Uni-ControlNet to the frameworks mentioned above and, additionally another state-of-the-art control framework, Composer[^12].
Composer requires training the model from scratch, which requires massive GPU resources and incurs a huge training cost.
In contrast, ControlNet, GLIGEN, and T2I-Adapter only require fine-tuning of lightweight adapters.
All of them need one independent adapter for each condition, resulting in a linear increase in fine-tuning cost.
Uni-ControlNet always requires the fine-tuning of only two adapters, no matter how many different conditions are used.
GLIGEN has the additional limitation that the composability of different conditions is not possible.

<p align="center">
    <img src="/images/comparison_cost_table.png" width="700px">
    <br>
    <sup>Comparison of fine-tuning cost (<a href="https://arxiv.org/pdf/2305.16322">source</a>)</sup>
</p>


Conclusion
======

Uni-ControlNet enables the integration of local and global controls.
It also allows for the simultaneous utilization of multiple conditions.
Uni-ControlNet surpasses alternatives like ControlNet, GLIGEN, and T2I-Adapter in controllability and quality while only requiring the fine-tuning of two adapters.
It is therefore more cost-effective and performs better than alternative frameworks.


References
======
[^1]: [DALL-E 3](https://openai.com/index/dall-e-3/)
[^2]: [Stable Diffusion 3](https://stability.ai/news/stable-diffusion-3)
[^3]: [Uni-ControlNet](https://arxiv.org/pdf/2305.16322)
[^4]: [Diffusion Models in Vision: A Survey](https://arxiv.org/pdf/2209.04747)
[^5]: [Gaussian noise](https://en.wikipedia.org/wiki/Gaussian_noise)
[^6]: [An intuitive introduction to text embeddings](https://stackoverflow.blog/2023/11/09/an-intuitive-introduction-to-text-embeddings/)
[^7]: [CLIP: Connecting text and images](https://openai.com/index/clip/)
[^8]: [ControlNet](https://arxiv.org/pdf/2302.05543)
[^9]: [GLIGEN](https://gligen.github.io/)
[^10]: [T2I-Adapter](https://arxiv.org/pdf/2302.08453)
[^11]: [Structural similarity index measure](https://en.wikipedia.org/wiki/Structural_similarity_index_measure#:~:text=The%20structural%20similarity%20index%20measure,the%20similarity%20between%20two%20images.)
[^12]: [Composer](https://arxiv.org/pdf/2302.09778)
