---
title: 'Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models - DRAFT'
date: 2024-05-15
permalink: /posts/2024/Uni-ControlNet/
tags:
  - Generative artificial intelligence
  - Text-to-Image generation
  - Diffusion models
---

Artificial intelligence (AI) has made tremendous progress over the last two years and has received a lot of attention from the media and the public.
The subcategory of AI that has seen the most progress is generative artificial intelligence.
Generative AI models are capable of generating new data (usually test, images or videos) in response to prompts given by the user.
Starting with the recent success of ChatGPT, some of these models have found their way into everyday life and are no longer just a subject of research.
Alongside chatbots, the most frequently used generative AI applications are Text-to-Image models like DALL-E, or Stable Diffusion.
These are able to generate realistic and detailed images using a text prompt given by the user.

In this blog post, I intend to explain a common weakness of Text-to-Image generation models and explore one possible solution following the introduction of Uni-ControlNet.

Why do we need more control in Text-to-Image models?
======

Text-to-Image models are great at generating detailed images that adhere to the specifications of a simple text prompt.
But if we want to create a specific image, it can be hard to write a text prompt that accurately describes our needs and results in the desired image being generated.

Let's say for example, we really like this wallpaper of a Lamborghini driving on a racetrack and want to recreate the same image with a different car.
We start with a simple prompt and let the model generate an image.
Then we add more details to the text prompt and try to influence the image generation to make the resulting image look more like our reference image.
We find that even with a very detailed description, it is difficult to directly influence the generated image.
Furthermore, the models face challenges in understanding complex text prompts.
In our example, the created images show a car making a right-hand bend, even though the prompt specified that the car should be taking a left turn.
Another problem are ambiguous sentences like "John watched the man with the telescope".
This could either mean that John was using the telescope to watch the man or that John was watching the man, while the man was using the telescope.
A Text-to-Image model may choose the wrong interpretation of such a sentence.

The above mentioned reasons show, that there is a need for more control beyond text descriptions.
This is the reason why Uni-ControlNet and similar technologies were developed.

![Wallpaper of a lamborghini](/images/lamborghini_wallpaper.jpg){: .align-center width="450px"}

| Image 1  | Image 2 | Image 3 |
| -------- | ------- | ------- |
| ![Image of LaFerrari 1](/images/laferrari_image_1.jpeg){: .align-left width="200px"} | ![Image of LaFerrari 2](/images/laferrari_image_2.jpeg){: .align-left width="200px"} | ![Image of LaFerrari 3](/images/laferrari_image_3.jpeg){: .align-left width="200px"} |
| A red LaFerrari is driving towards the camera. | A red LaFerrari is driving towards the camera. The car is driving on a racetrack and is taking a left turn. | A red LaFerrari is driving towards the camera. The car is driving on a racetrack and is taking a left turn. In the background, you can see two modern-looking buildings and stacked tires that are painted red and white.|

What is Uni-ControlNet?
======

Uni-ControlNet is a framework that allows for the utilization of different conditions beyond text descriptions.
These conditions are categorized into local and global controls.
Local conditions are additional maps, like for example edge or depth maps, on which the final result is based.
On the other hand, a global condition is a reference image that the diffusion model takes into account.

Uni-ControlNet works with pre-trained text-to-Image diffusion models.
There is no need to train a new model. 
An existing model can be augmented with Uni-ControlNet without changing the model itself.
This expansion of the model is realized using two adapters, one for local and one for global control.
In the implementation considered here, the Text-to-Image model Stable Diffusion is used, although this approach also works for other diffusion models.

Here you can see an example of a local condition being used to control the generation of an image. 
The prompt "A man running on the street" is extended with a condition that describes the position of his body parts.

![Motivation controls](/images/motivation_controls.png){: .align-center width="800px"}


What are diffusion models and how do they work?
======

Diffusion models are a recent development in the field of generative AI. 
They were first proposed in 2015, but found brought adoption only in the last two years.
Diffusion models are inspired by thermodynamics and are used in cutting-edge Text-to-Image models like DALL-E and Stable Diffusion.

![Number of papers diffusion](/images/number_of_papers_diffusion.png){: .align-center width="500px"}

Diffusion models work by first systematically and slowly destroying structure in the training data.
Since we are talking about image generation, the training data is a collection of images.
We can destroy the structure in these images by applying low-level noise.
This process is called Forward Diffusion.
Then we try to train a model that can reverse the process and recover structure from the data.
We call this step Reverse Diffusion Process.
At inference time, we can generate images by passing random noise through the recovering process and using the text prompt to create a new image.
In the following section, we will take a more detailed look at each of the different steps.

![Diffusion overview](/images/diffusion_overview.png){: .align-center width="500px"}

Forward Diffusion Process
------

The Forward Diffusion process consists of multiple steps of adding Gaussian noise to the input image.
The scale of noise varies at each step.
By applying noise, the training data is progressively destroyed.
At the end of this process, only pure Gaussian noise remains.

![Forward diffusion process](/images/forward_diffusion_process.png){: .align-center width="500px"}

Reverse Diffusion Process
------

The goal of the Reverse Diffusion Process is to train a model to predict the noise in a given image.
We can then subtract the noise from the image to recover more of the original image.
We repeat this process multiple times until we have completely reversed the diffusion process.
Now we have a model that can recover images from random noise.

![Reverse diffusion process](/images/reverse_diffusion_process.png){: .align-center width="500px"}

Diffusion Inference
------

To create a new image, we start with random white noise.
We then gradually try to reconstruct an image by removing noise using the Reverse Diffusion Process.
Text embeddings are extracted from the user prompt and used for image generation.

![Diffusion inference](/images/diffusion_inference.png){: .align-center width="500px"}


How does Uni-ControlNet work?
======

Uni-ControlNet doesn't change the base diffusion process.
It adds two adapters to the image generation process, one for local and one for global control.
The global control adapter takes the text prompt and the global condition and produces an extended prompt.
The local control adapter uses noise, the local conditions, and the extended prompt to produce an output that is combined with the output from the Diffusion Encoder.
Let's take a closer look at how the adapters work.

![Framework Uni-ControlNet](/images/framework_uni-controlnet.png){: .align-center width="800px"}

Global Control Adapter
------

Global control is realized with a reference image.
This provides a more nuanced understanding of the semantic content.

![Global control example](/images/global_control_example.png){: .align-center width="500px"}

At first image embeddings are extracted using a tool called CLIP.
CLIP is an image encoder developed by OpenAI.
Image embeddings encode the contents of an image.
We then reshape these embeddings into k tokens.

Concatenate the tokens from the text prompt with the condition tokens.
This results in the extended prompt.

![Global control adapter](/images/global_control_adapter.png){: .align-center width="500px"}

![Weight of global condition](/images/weight_of_global_condition.png){: .align-center width="500px"}

Local Control Adapter
------

Uni-ControlNet supports seven different local conditions.
These conditions are Canny edge, MLSD edge, HED boundary, sketch, Openpose, Midas depth, and segmentation mask.

![Local control adapters](/images/local_control_adapters.png){: .align-center width="500px"}

The local control adapter starts by concatenating the different local conditions.
After this, the Feature Extractor H is used to extract features.
The structure and weights of the encoder from the diffusion model are copied and also used in this adapter.
The condition information is injected at different resolutions into the copied encoder.

![Local control adapter](/images/local_control_adapter.png){: .align-center width="500px"}

Information from the local control adapter is combined with the output of the encoder and used during decoding.
The two adapters are trained separately and merged at inference time.

Composition of conditions
------

Multiple local and one global condition can be combined and are simultaneously respected by the diffusion model.
This doesn't work for all combinations of local conditions because they may be contradictory.

![Composition of conditions](/images/composition_of_conditions.png){: .align-center width="700px"}


Comparison with related work
======

In the following, Uni-NetControl is compared to other state-of-the-art control frameworks for Text-to-Image generation.
These are ControlNet, GLIGEN and T2I-Adapters.
For the given text prompt and local conditions, the frameworks produce different results.

![Comparison of quality](/images/comparison_quality.png){: .align-center width="700px"}

To quantify how closly the frameworks followed the given conditions, we first take a natural image and automatically extract our condition (e.g. Canny edge map).
We then take this condition and the corresponding text prompt and create images with each different framework.
From the generated images, we also extract the condition and compare it to the condition of our ground truth.
For this comparison, we use SSIM (Structural Similarity).

![Comparison of quality table](/images/comparison_quality_table.png){: .align-center width="700px"}

To further compare the different frameworks, let's take a look at the cost and model size.
Composer requires training the model from scratch, which requires massive GPU resources and incurs huge training cost.
In contrast ControlNet, GLIGEN and T2I-Adapter only require fine-tuning of lightweight adapters.
They need one independent adapter for each condition, resulting in a linear increase in fine-tuning cost.
The composability of different conditions is not possible with GLIGEN.

![Comparison of cost table](/images/comparison_cost_table.png){: .align-center width="700px"}


Conclusion
======

Uni-ControlNet enables the integration of local and global controls.
It also enables simultaneous utilization of multiple conditions.
Uni-ControlNet surpasses alternatives like ControlNet, GLIGEN, and T2I-Adapter in controllability and quality,
while only requiring fine-tuning of two adapters.
It is therefore more cost-effective than other frameworks.

