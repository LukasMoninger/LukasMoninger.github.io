---
title: 'Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models'
date: 2024-05-15
permalink: /posts/2024/05/Uni-ControlNet/
tags:
  - Text-to-Image generation
  - Diffusion models
---

Generative AI has made tremendous progress over the last two years.
Alongside chatbots, the most attention was paid to Text-to-Image diffusion models like OpenAI's DALL-E and Stable Diffusion.
They are able to create realistic and detailed images from a text prompt.

Why do we need more control in text-to-Image models?
======

If we want to create a specific image we can find it hard to write text promts that accuratly descripe the image and result in the desired image being generated.
Let's say we really like this wallpaper of the lamborghini driving on the racetrack and want to recreate it with a different car.
We can start with a simple promt and let the model create an image. Then we can add some more details to try and influence the image generation.
We find that even with a very detailed description it is difficult to directly influence details in the generated image.
Furthermore the models face challenges in understanding complex text prompts.
The above mentioned reasons show that there is a need for more control beyond text descriptions, which is the reason why Uni-ControlNet was introduced.

![Wallpaper of a lamborghini](/images/lamborghini_wallpaper.jpg){: .align-right width="400px"}
Picture of a Lamborghini

![Image of LaFerrari 1](/images/laferrari_image_1.jpeg){: .align-left width="400px"}
*Create an image of a red LaFerrari driving towards the camera*
![Image of LaFerrari 2](/images/laferrari_image_2.jpeg){: width="400px"}
*A red LaFerrari driving towards the camera. The car is driving on a racetrack and is taking a left turn.*
![Image of LaFerrari 3](/images/laferrari_image_3.jpeg){: width="400px"}
*Create an image of a red LaFerrari driving towards the camera. The car is driving on a racetrack and is in a left turn. 
In the background, you can see two modern-looking buildings and stacked tires that are painted red and white.*

Whas is Uni-ControlNet?
======

Uni-ControlNet is a framework that allows the utilization of local and global controls.
It works with pre-trained text-to-Image diffusion models. In this case Stable Diffusion is used.
Only requires fine-tuning of two adapters.

![Motivation controls](/images/motivation_controls.png){: .align-left width="800px"}

What are diffusion models and how do they work?
======

Diffusion models are a very recent development in the field of generative AI. 
They were first proposed in 2015, but found broudt adaption only in the last two years.
they are used in cutting edge Text-to-Image models like DALL-E and Stable Diffusion.
Diffusion models are inspired by thermodynamics.

![Number of papers diffusion](/images/number_of_papers_diffusion.png){: .align-right width="500px"}

Systematically and slowly destroy structure in training data
Learn a process that recovers structure in data
Generate images by passing random noise through recovering process

![Diffusion overview](/images/diffusion_overview.png){: .align-left width="500px"}

Forward Diffusion Process
------

Multiple steps of adding Gaussian noise to input image
Scale of noise varies at each step
The training data is progressively destroyed

![Forward diffusion process](/images/forward_diffusion_process.png){: .align-left width="500px"}

Reverse Diffusion Process
------

Train a model to predict the noise in a given image
Subtract the noise from the image
Reverse the diffusion process in multiple steps

![Reverse diffusion process](/images/reverse_diffusion_process.png){: .align-left width="500px"}

Diffusion Inference
------

Start with random white noise
Gradually try to reconstruct image by removing noise
Use text embeddings from user prompt

![Diffusion inference](/images/diffusion_inference.png){: .align-left width="500px"}

How does Uni-ControlNet work?
======

Uni-ControlNet adds two adapters to the image generation process.

![Framework Uni-ControlNet](/images/framework_uni-controlnet.png){: .align-left width="500px"}

Global Control Adapter
------

Global control is realized with a reference image
Provides a more nuanced understanding of the semantic content

Extract image embeddings using CLIP (image encoder by OpenAI)
Image embeddings encode the contents of an image
Reshape the embeddings into k tokens

Concatenate the text prompt with the condition tokens
Result is extended prompt

Local Control Adapter
------

Uni-ControlNet supports seven different local conditions

Concatenate different local conditions and extract features
Copy the structure and weights of the encoder
Inject the condition information at different resolutions

Information from local control adapter is usedduring decoding
The two adapters are trained separately and merged at inference time

Composition of conditions
------

Multiple local and one global condition can be combined

Comparison with related work
======

Extract conditions from natural and generated image
Compare using SSIM (Structural Similarity)

Composer requirestraining the model from scratch
ControlNet, GLIGENandT2I-Adapter need one independent adapter for each condition
Composability of different conditions not possible with GLIGEN

Conclusion
======

Uni-ControlNet enables integration of local and global controls
Simultaneousutilizationofmultipleconditionsispossible
Surpasses ControlNet, GLIGEN and T2I-Adapter in controllability and quality
Uni-ControlNet only requires fine-tuning of two adapters

![Weight of global condition](/images/weight_of_global_condition.png){: .align-left width="500px"}
![Local control adapters](/images/local_control_adapters.png){: .align-left width="500px"}
