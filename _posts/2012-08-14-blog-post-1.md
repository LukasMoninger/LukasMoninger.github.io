---
title: 'Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models'
date: 2024-05-15
permalink: /posts/2024/05/Uni-ControlNet/
tags:
  - Text-to-Image generation
  - Diffusion models
---

Generative AI has made tremendous progress over the last two years.
Driven by the success of chatGPt there has been a lot of media attention on the subject of generative AI.
Alongside chatbots, the most attention was paid to Text-to-Image diffusion models like OpenAI's DALL-E and Stable Diffusion.
They are able to create realistic and detailed images from a text prompt.

Why do we need more control in text-to-Image models?
======

If we want to create a specific image we can find it hard to write text promts that accuratly descripe the image and result in the desired image being generated.
Let's say we really like this wallpaper of the lamborghini driving on the racetrack and want to recreate it with a different car.
We can start with a simple promt and let the model create an image. Then we can add some more details to try and influence the image generation.
We find that even with a very detailed description it is difficult to directly influence details in the generated image.
Furthermore the models face challenges in understanding complex text prompts.
The above mentioned reasons show that there is a need for more control beyond text descriptions, which is the reason why Uni-ControlNet was introduced.

![Wallpaper of a lamborghini](/images/lamborghini_wallpaper.jpg){: .align-center width="450px"}

![Image of LaFerrari 1](/images/laferrari_image_1.jpeg){: .align-left width="200px"}
<figcaption>Create an image of a red LaFerrari driving towards the camera</figcaption>

![Image of LaFerrari 2](/images/laferrari_image_2.jpeg){: width="200px"}
<figcaption>A red LaFerrari driving towards the camera. The car is driving on a racetrack and is taking a left turn.</figcaption>

![Image of LaFerrari 3](/images/laferrari_image_3.jpeg){: width="200px"}
<figcaption>Create an image of a red LaFerrari driving towards the camera. The car is driving on a racetrack and is in a left turn. 
In the background, you can see two modern-looking buildings and stacked tires that are painted red and white.</figcaption>


What is Uni-ControlNet?
======

Uni-ControlNet is a framework that allows the utilization of local and global controls.
It works with pre-trained text-to-Image diffusion models. In this case Stable Diffusion is used.
Only requires fine-tuning of two adapters.

![Motivation controls](/images/motivation_controls.png){: .align-center width="800px"}


What are diffusion models and how do they work?
======

Diffusion models are a very recent development in the field of generative AI. 
They were first proposed in 2015, but found broudt adaption only in the last two years.
They are used in cutting edge Text-to-Image models like DALL-E and Stable Diffusion.
Diffusion models are inspired by thermodynamics.

![Number of papers diffusion](/images/number_of_papers_diffusion.png){: .align-center width="500px"}

Diffusion models work, by first systematically and slowly destroying structure in the training data.
Since we are talking about image generation, the training data is a collection of images.
We can destroy structure in these images by applying a low-level noise.
This process is called Forward Diffusion.
Then we try to train a model that can reverse the process and recovers structure in data.
We call this the Reverse Diffusion Process.
At inference time we can generate images by passing random noise through recovering process
and using the text prompt to create a new image.

![Diffusion overview](/images/diffusion_overview.png){: .align-center width="500px"}

Forward Diffusion Process
------

The Forward Diffusion process consists of multiple steps of adding Gaussian noise to the input image.
The scale of noise varies at each step.
by applying noise the training data is progressively destroyed.
At the end of this process only static noise remains.

![Forward diffusion process](/images/forward_diffusion_process.png){: .align-center width="500px"}

Reverse Diffusion Process
------

The goal of the Reverse Diffusion Process is, to train a model to predict the noise in a given image.
We can then subtract the noise from the image to recover more of the original image.
We repeat this process multiple steps until we have completly reversed the diffusion process.
Now we have a model that can recover images from randm noise.

![Reverse diffusion process](/images/reverse_diffusion_process.png){: .align-center width="500px"}

Diffusion Inference
------

To create a new image, we start with random white noise.
We then gradually try to reconstruct a image by removing noise using the Reverse Diffusion Process.
Use text embeddings from user prompt.

![Diffusion inference](/images/diffusion_inference.png){: .align-center width="500px"}


How does Uni-ControlNet work?
======

Uni-Controlnet doesn't change the base diffusion process.
It adds two adapters to the image generation process.
One adapter is used for global control and the other is used for local control.
the global control adapter takes the text prompt and the global conditions and produces an extendet promt.
This extendet prompt is used in the further image generation.
Let's take a closer look at how the adapters work.

![Framework Uni-ControlNet](/images/framework_uni-controlnet.png){: .align-center width="500px"}

Global Control Adapter
------

Global control is realized with a reference image.
Provides a more nuanced understanding of the semantic content.

![Global control example](/images/global_control_example.png){: .align-center width="500px"}

Extract image embeddings using CLIP (image encoder by OpenAI)
Image embeddings encode the contents of an image
Reshape the embeddings into k tokens

Concatenate the text prompt with the condition tokens
Result is extended prompt

![Global control adapter](/images/global_control_adapter.png){: .align-center width="500px"}

![Weight of global condition](/images/weight_of_global_condition.png){: .align-center width="500px"}

Local Control Adapter
------

Uni-ControlNet supports seven different local conditions

![Local control adapters](/images/local_control_adapters.png){: .align-left width="500px"}

Concatenate different local conditions and extract features
Copy the structure and weights of the encoder
Inject the condition information at different resolutions

![Local control adapter](/images/local_control_adapter.png){: .align-center width="500px"}

Information from local control adapter is usedduring decoding
The two adapters are trained separately and merged at inference time

Composition of conditions
------

Multiple local and one global condition can be combined

![Composition of conditions](/images/composition_of_conditions.png){: .align-center width="700px"}


Comparison with related work
======

In the following Uni-NetControl is compared to other state-of-the-art control frameworks for Text-to-Image generation.
These are ControlNet, GLIGEN and T2I-Adapters.
For the given text prompt and local conditions the frameworks product different results.

![Comparison of quality](/images/comparison_quality.png){: .align-center width="700px"}

Extract conditions from natural and generated image
Compare using SSIM (Structural Similarity)

![Comparison of quality table](/images/comparison_quality_table.png){: .align-center width="700px"}

To further compare the different frameworks, lets take a look at the cost and model size.
Composer requires training the model from scratch.
ControlNet, GLIGENandT2I-Adapter need one independent adapter for each condition.
Composability of different conditions not possible with GLIGEN.

![Comparison of cost table](/images/comparison_cost_table.png){: .align-center width="700px"}


Conclusion
======

Uni-ControlNet enables integration of local and global controls.
Simultaneousutilizationofmultipleconditionsispossible.
Surpasses ControlNet, GLIGEN and T2I-Adapter in controllability and quality.
Uni-ControlNet only requires fine-tuning of two adapters.

